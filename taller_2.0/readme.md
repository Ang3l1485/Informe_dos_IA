# Proyecto Actividad 2 - Machine Learning Supervisado

## 3. Modelo 1: Random Forest

Decidimos utilizar el algoritmo **Random Forest**, el cual consiste en
entrenar m√∫ltiples √°rboles de decisi√≥n independientes que trabajan de
manera conjunta. Cada √°rbol realiza predicciones basadas en una muestra
diferente de los datos y un subconjunto aleatorio de variables, y al
final, el modelo combina todas esas predicciones (votaci√≥n en
clasificaci√≥n o promedio en regresi√≥n) para obtener un resultado m√°s
robusto y preciso.\
Este enfoque ayuda a reducir el sobreajuste y mejora la capacidad de
generalizaci√≥n del modelo.

En nuestro caso, se configur√≥ con **200 √°rboles** y el criterio de
partici√≥n **Gini**, el cual mide la impureza de las divisiones en las
hojas de los √°rboles. Elegimos **Gini** sobre *Entropy* ya que ofrece
resultados similares, pero con un menor costo computacional, logrando un
buen balance entre velocidad y precisi√≥n.

### Par√°metros utilizados:

``` python
RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    n_jobs=-1,
    criterion="gini",
    max_features='sqrt'
)
```

### Resultados obtenidos

Los resultados del modelo sobre el conjunto de prueba fueron los
siguientes:

  M√©trica                                 Valor
  --------------------------------------- ------------
  Exactitud (Accuracy)                    **0.9022**
  Precisi√≥n (Precision)                   **0.8962**
  Sensibilidad / Exhaustividad (Recall)   **0.9314**
  Puntaje F1 (F1-score)                   **0.9135**

### Interpretaci√≥n

-   **Exactitud (Accuracy):** El modelo clasifica correctamente el
    **90.22%** de los casos.\
-   **Precisi√≥n:** De todos los pacientes que el modelo predijo como
    positivos (con enfermedad), el **89.62%** realmente lo son.\
-   **Sensibilidad (Recall):** El modelo detecta correctamente el
    **93.14%** de los pacientes enfermos, lo que es crucial en un
    problema m√©dico.\
-   **F1-score:** Con un balance entre precisi√≥n y recall, el modelo
    alcanza un **91.35%**, mostrando un desempe√±o consistente y
    equilibrado.

## Red neuronal


# la parte de mi compa√±ero va aqu√≠ j



# An√°lisis del modelo Gradient Boosting

## Explicaci√≥n del algoritmo
El **Gradient Boosting** es un algoritmo de ensamble que combina m√∫ltiples modelos d√©biles (generalmente √°rboles de decisi√≥n poco profundos) de forma secuencial.  
A diferencia de *Random Forest*, que construye √°rboles de manera independiente y vota por mayor√≠a, Gradient Boosting entrena cada nuevo √°rbol corrigiendo los errores del √°rbol anterior.  
Esto se logra calculando los **residuos** (errores) del modelo anterior y ajustando el siguiente √°rbol para reducirlos. As√≠, el modelo va mejorando paso a paso hasta alcanzar una mejor capacidad de predicci√≥n.

En resumen:
- **Random Forest**: muchos √°rboles entrenados en paralelo ‚Üí votan en conjunto.  
- **Gradient Boosting**: muchos √°rboles entrenados en secuencia ‚Üí cada √°rbol corrige al anterior.
- **red neuronal**: manejado con pesos y capas.
---

## Par√°metros utilizados en la primera configuraci√≥n
```python
GradientBoostingClassifier(
    n_estimators=300,   # n√∫mero de √°rboles peque√±os entrenados en secuencia
    learning_rate=0.05, # tasa de aprendizaje, controla cu√°nto corrige cada √°rbol
    max_depth=3,        # profundidad m√°xima de cada √°rbol d√©bil
    random_state=42
)
```

### Explicaci√≥n de los hiperpar√°metros
- **n_estimators**: n√∫mero de √°rboles en la secuencia. M√°s √°rboles permiten un aprendizaje m√°s detallado, pero tambi√©n aumentan el riesgo de sobreajuste y el costo computacional.  
- **learning_rate**: controla cu√°nto aporta cada √°rbol al modelo final.  
  - Un valor alto = aprendizaje r√°pido, pero riesgo de sobreajuste.  
  - Un valor bajo = aprendizaje m√°s estable, requiere m√°s √°rboles.  
- **max_depth**: controla qu√© tan complejos son los √°rboles individuales.  
  - √Årboles profundos ‚Üí mayor capacidad de ajuste, pero riesgo de sobreajuste.  
  - √Årboles superficiales ‚Üí m√°s estables, pero necesitan m√°s √°rboles para lograr buena precisi√≥n.

---

## Resultados iniciales
Resultados obtenidos con `n_estimators=300`, `learning_rate=0.05`, `max_depth=3`:

üìä *[Aqu√≠ va la URL de la imagen de la matriz de confusi√≥n inicial]*

---

## Segunda configuraci√≥n optimizada
Despu√©s de varias pruebas, se encontr√≥ que lo mejor era **reducir el learning_rate y aumentar el n√∫mero de √°rboles**, manteniendo √°rboles poco profundos.  
Esto permite que el modelo se acerque en pasos peque√±os al resultado deseado, reduciendo el riesgo de sobreajuste.

Configuraci√≥n:
```python
GradientBoostingClassifier(
    n_estimators=700,   # m√°s √°rboles, cada uno corrige un peque√±o error
    learning_rate=0.005,# pasos peque√±os y estables
    max_depth=2,        # √°rboles muy simples para evitar sobreajuste
    random_state=42
)
```

---

## Resultados finales
üìä *[Aqu√≠ va la URL de la imagen de la matriz de confusi√≥n final]*

| M√©trica              | Valor   |
|----------------------|---------|
| **Exactitud (Accuracy)** | 0.8804 |
| **Precisi√≥n (Precision)** | 0.8704 |
| **Sensibilidad (Recall)** | 0.9216 |
| **F1-score**            | 0.8952 |

---

## Conclusiones de estos resultados
- Se observ√≥ que al **incrementar demasiado la profundidad o el n√∫mero de √°rboles** el modelo ca√≠a en **sobreajuste** (overfitting), es decir, memorizaba demasiado los datos de entrenamiento y perd√≠a capacidad de generalizaci√≥n.  
- Con la configuraci√≥n optimizada (muchos √°rboles poco profundos y learning_rate bajo), el modelo logr√≥ un **buen equilibrio entre precisi√≥n y generalizaci√≥n**.  
- Aunque el rendimiento fue ligeramente inferior al de Random Forest en exactitud global, Gradient Boosting mostr√≥ una **mayor estabilidad en la detecci√≥n de pacientes con la condici√≥n (Recall = 0.92)**, lo cual puede ser m√°s relevante en contextos m√©dicos.

En conclusi√≥n, **Gradient Boosting es un algoritmo poderoso cuando se ajustan correctamente sus hiperpar√°metros**. Su principal ventaja frente a Random Forest es que puede capturar relaciones m√°s complejas y corregir errores paso a paso, pero a cambio requiere un ajuste m√°s cuidadoso para evitar sobreajuste.

